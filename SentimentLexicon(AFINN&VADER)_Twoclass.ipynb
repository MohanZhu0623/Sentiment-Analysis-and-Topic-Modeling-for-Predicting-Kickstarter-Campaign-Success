{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMo6iw/m/882SJOOSak8vem",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MohanZhu0623/Sentiment_Analysis/blob/main/SentimentLexicon(AFINN%26VADER)_Twoclass.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TNsine51Zflj",
        "outputId": "e3a6461f-7b2d-4939-fc5c-a51d85268e70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting afinn\n",
            "  Downloading afinn-0.1.tar.gz (52 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/52.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.6/52.6 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: afinn\n",
            "  Building wheel for afinn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for afinn: filename=afinn-0.1-py3-none-any.whl size=53430 sha256=78a41eace445792330edaf78afb4488803f38931dd6cafc4cdc779cfb66b2e52\n",
            "  Stored in directory: /root/.cache/pip/wheels/b0/05/90/43f79196199a138fb486902fceca30a2d1b5228e6d2db8eb90\n",
            "Successfully built afinn\n",
            "Installing collected packages: afinn\n",
            "Successfully installed afinn-0.1\n",
            "Collecting vaderSentiment\n",
            "  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl.metadata (572 bytes)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from vaderSentiment) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (2024.7.4)\n",
            "Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.0/126.0 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: vaderSentiment\n",
            "Successfully installed vaderSentiment-3.3.2\n"
          ]
        }
      ],
      "source": [
        "!pip install afinn\n",
        "!pip install vaderSentiment\n",
        "import pandas as pd\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from afinn import Afinn\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "0nPkT9NsaSIN"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SjhZnIXRaHNz",
        "outputId": "959d8472-6f0c-4cbc-a6b9-eead2d3b32c3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "data = pd.read_excel('/content/binary_labelled _data.xlsx')"
      ],
      "metadata": {
        "id": "7N-JGn9DZgtt"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess data\n",
        "def pre_process_data(dataset):\n",
        "    # Convert to lowercase\n",
        "    dataset['text'] = dataset['text'].str.lower()\n",
        "    # Remove numbers\n",
        "    dataset['text'] = dataset['text'].str.replace(r'\\d+', '', regex=True)\n",
        "    # Remove punctuation\n",
        "    dataset['text'] = dataset['text'].str.replace(f\"[{string.punctuation}]\", \"\", regex=True)\n",
        "    # Remove whitespace\n",
        "    dataset['text'] = dataset['text'].str.strip()\n",
        "    # Remove stopwords except 'not'\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    stop_words.remove('not')\n",
        "    dataset['text'] = dataset['text'].apply(lambda x: ' '.join([word for word in word_tokenize(x) if word not in stop_words]))\n",
        "    return dataset\n",
        "\n",
        "# Apply preprocessing to the data\n",
        "data = pre_process_data(data)\n",
        "print(data.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kjKSWE3maAsD",
        "outputId": "51065b90-008e-477f-f453-4061d609ca42"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                text  class\n",
            "0  project designed help protect environment usin...      1\n",
            "1  help us built sustainable studio eliminate cla...      1\n",
            "2  free app allow pool reservations others get gr...      1\n",
            "3  prohibition themed gastro pub dark silent head...      1\n",
            "4  sean naturally talented trumpet player ryan wo...      1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the labeled data into training set (80%) and hold-out test set (20%)\n",
        "partition = 0.8\n",
        "train_data, test_data = train_test_split(data, test_size=1-partition, random_state=128, stratify=data['class'])\n",
        "print(f\"Training set size: {train_data.shape}\")\n",
        "print(f\"Test set size: {test_data.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g4c34zbSaqjT",
        "outputId": "9da64bc6-0655-4428-8b61-844816c839d6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set size: (1051, 2)\n",
            "Test set size: (263, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to classify text using AFINN\n",
        "afinn = Afinn()\n",
        "\n",
        "def classify_afinn(text):\n",
        "    score = afinn.score(text)\n",
        "    if score > 0:\n",
        "        return 1\n",
        "    else:\n",
        "        return 0"
      ],
      "metadata": {
        "id": "7rSri_6uaat5"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to classify text using VADER\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "def classify_vader(text):\n",
        "    scores = analyzer.polarity_scores(text)\n",
        "    compound = scores['compound']\n",
        "    if compound > 0.05:\n",
        "        return 1\n",
        "    else:\n",
        "        return 0"
      ],
      "metadata": {
        "id": "eha36Q4Racy0"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply classification to the test set using AFINN\n",
        "test_data['afinn_predicted'] = test_data['text'].apply(classify_afinn)\n",
        "\n",
        "# Apply classification to the test set using VADER\n",
        "test_data['vader_predicted'] = test_data['text'].apply(classify_vader)"
      ],
      "metadata": {
        "id": "Ls3pK5_3aevY"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate metrics for AFINN\n",
        "afinn_accuracy = accuracy_score(test_data['class'], test_data['afinn_predicted'])\n",
        "afinn_precision = precision_score(test_data['class'], test_data['afinn_predicted'], average='binary')\n",
        "afinn_recall = recall_score(test_data['class'], test_data['afinn_predicted'], average='binary')\n",
        "afinn_f1 = f1_score(test_data['class'], test_data['afinn_predicted'], average='binary')\n",
        "\n",
        "# Calculate metrics for VADER\n",
        "vader_accuracy = accuracy_score(test_data['class'], test_data['vader_predicted'])\n",
        "vader_precision = precision_score(test_data['class'], test_data['vader_predicted'], average='binary')\n",
        "vader_recall = recall_score(test_data['class'], test_data['vader_predicted'], average='binary')\n",
        "vader_f1 = f1_score(test_data['class'], test_data['vader_predicted'], average='binary')\n",
        "\n",
        "# Print the results\n",
        "print(\"AFINN Metrics:\")\n",
        "print(f\"Accuracy: {afinn_accuracy}\")\n",
        "print(f\"Precision: {afinn_precision}\")\n",
        "print(f\"Recall: {afinn_recall}\")\n",
        "print(f\"F1 Score: {afinn_f1}\")\n",
        "\n",
        "print(\"\\nVADER Metrics:\")\n",
        "print(f\"Accuracy: {vader_accuracy}\")\n",
        "print(f\"Precision: {vader_precision}\")\n",
        "print(f\"Recall: {vader_recall}\")\n",
        "print(f\"F1 Score: {vader_f1}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2oup6D7wawq0",
        "outputId": "5233b291-59a0-428a-adf6-d64bc98df110"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AFINN Metrics:\n",
            "Accuracy: 0.7718631178707225\n",
            "Precision: 0.9130434782608695\n",
            "Recall: 0.7616580310880829\n",
            "F1 Score: 0.8305084745762711\n",
            "\n",
            "VADER Metrics:\n",
            "Accuracy: 0.844106463878327\n",
            "Precision: 0.9130434782608695\n",
            "Recall: 0.8704663212435233\n",
            "F1 Score: 0.8912466843501325\n"
          ]
        }
      ]
    }
  ]
}