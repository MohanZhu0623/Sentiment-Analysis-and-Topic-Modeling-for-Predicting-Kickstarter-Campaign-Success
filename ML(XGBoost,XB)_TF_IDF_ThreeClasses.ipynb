{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyME/syjSqMt04ZYudpln8oK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MohanZhu0623/Sentiment_Analysis/blob/main/ML(XGBoost%2CXB)_TF_IDF_ThreeClasses.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rgY01GDlB_fB"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import string\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, RepeatedKFold\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split, RepeatedKFold\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix"
      ],
      "metadata": {
        "id": "IoAF0pCvL5fV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ilPoCbGVL-Pr",
        "outputId": "775a7a6a-beb1-4b2d-c8c8-11cb3e178a3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_excel('/content/python_labelled_data.xlsx')"
      ],
      "metadata": {
        "id": "rwzsnYa1MBrU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess data\n",
        "def pre_process_data(dataset):\n",
        "    # Convert to lowercase\n",
        "    dataset['text'] = dataset['text'].str.lower()\n",
        "    # Remove numbers\n",
        "    dataset['text'] = dataset['text'].str.replace(r'\\d+', '', regex=True)\n",
        "    # Remove punctuation\n",
        "    dataset['text'] = dataset['text'].str.replace(f\"[{string.punctuation}]\", \"\", regex=True)\n",
        "    # Remove whitespace\n",
        "    dataset['text'] = dataset['text'].str.strip()\n",
        "    # Remove stopwords except 'not'\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    stop_words.remove('not')\n",
        "    dataset['text'] = dataset['text'].apply(lambda x: ' '.join([word for word in word_tokenize(x) if word not in stop_words]))\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "qjatq36wMDI4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply preprocessing to the data\n",
        "data = pre_process_data(data)\n",
        "print(data.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QLNib56TMEVC",
        "outputId": "a482a2c0-b0b7-4933-a76e-8f1219369fae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                text  class\n",
            "0  project designed help protect environment usin...      1\n",
            "1  help us built sustainable studio eliminate cla...      1\n",
            "2  paint something dont want explain isbob ross b...      0\n",
            "3  free app allow pool reservations others get gr...      1\n",
            "4  prohibition themed gastro pub dark silent head...      1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create TF-IDF vectorizer for unigrams and bigrams\n",
        "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 2), min_df=3)\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(data['text'])\n",
        "\n",
        "# Convert to DataFrame\n",
        "corpus_clean = pd.DataFrame(X_tfidf.toarray(), columns=tfidf_vectorizer.get_feature_names_out())"
      ],
      "metadata": {
        "id": "vu2gIID8MFit"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine class label with features\n",
        "labeled_dtm = pd.concat([data[['class']], corpus_clean], axis=1)"
      ],
      "metadata": {
        "id": "tpdcmruoMHEU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "labeled_dtm = labeled_dtm.loc[:, ~labeled_dtm.columns.duplicated()]\n",
        "\n",
        "print(labeled_dtm.columns.duplicated().sum())\n",
        "class_column = labeled_dtm['class'].squeeze()\n",
        "print(class_column.shape)\n",
        "print(class_column.value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tBpTeYYLMIUa",
        "outputId": "9cfce34f-0a7e-4a8d-de92-95f9a0379e87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "(2269,)\n",
            "class\n",
            " 1    962\n",
            " 0    955\n",
            "-1    352\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the labels to non-negative integers\n",
        "labeled_dtm['class'] = labeled_dtm['class'].map({-1: 0, 0: 1, 1: 2})\n",
        "\n",
        "# Split the labeled DTM into training set (80%) and hold-out test set (20%)\n",
        "partition = 0.8\n",
        "train_labeled, test_labeled = train_test_split(labeled_dtm, test_size=1-partition, random_state=128, stratify=labeled_dtm['class'])\n",
        "print(f\"Training set size: {train_labeled.shape}\")\n",
        "print(f\"Test set size: {test_labeled.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z9ygvCgBMJaE",
        "outputId": "7c747838-cf75-4d16-da42-a61a8926f4fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set size: (1815, 2328)\n",
            "Test set size: (454, 2328)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set cross-validation parameters\n",
        "cv_tune = 5\n",
        "rep_tune = 1\n",
        "cv_final = 10\n",
        "rep_final = 5\n",
        "\n",
        "# Define cross-validation strategies\n",
        "cv_strategy_tune = RepeatedKFold(n_splits=cv_tune, n_repeats=rep_tune, random_state=128)\n",
        "cv_strategy_final = RepeatedKFold(n_splits=cv_final, n_repeats=rep_final, random_state=128)"
      ],
      "metadata": {
        "id": "TlgQDE9dMSu2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define parameter grid for XGBoost\n",
        "param_grid_xgb = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [5, 10, 15],\n",
        "    'learning_rate': [0.01, 0.1, 0.2]\n",
        "}\n",
        "\n",
        "# Define parameter grid for Naive Bayes\n",
        "param_grid_nb = {\n",
        "    'alpha': [0.1, 0.5, 1.0, 5.0, 10.0]\n",
        "}\n",
        "\n",
        "# Initialize XGBoost and Naive Bayes models\n",
        "xgb = XGBClassifier()\n",
        "nb = MultinomialNB()\n",
        "\n",
        "# Combine models and parameter grids\n",
        "models = {\n",
        "    'xgb': (xgb, param_grid_xgb),\n",
        "    'nb': (nb, param_grid_nb)\n",
        "}\n",
        "\n",
        "# Initialize results list\n",
        "results = []\n",
        "\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score\n",
        "# Define a function to evaluate the model on the test set\n",
        "def evaluate_model_on_test(model, X_test, y_test):\n",
        "    predictions = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, predictions)\n",
        "    f1 = f1_score(y_test, predictions, average='weighted')\n",
        "    precision = precision_score(y_test, predictions, average='weighted')\n",
        "    recall = recall_score(y_test, predictions, average='weighted')\n",
        "    conf_matrix = confusion_matrix(y_test, predictions)\n",
        "    return accuracy, f1, precision, recall, conf_matrix"
      ],
      "metadata": {
        "id": "hOvunKOyMLNI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train and tune models\n",
        "for model_name, (model, param_grid) in models.items():\n",
        "    print(f\"Training {model_name}...\")\n",
        "    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=cv_strategy_tune, verbose=2, n_jobs=-1)\n",
        "    start_time = time.time()\n",
        "    grid_search.fit(train_labeled.drop(columns=['class']), train_labeled['class'])\n",
        "    end_time = time.time()\n",
        "\n",
        "    # Save results\n",
        "    best_model = grid_search.best_estimator_\n",
        "    train_acc = grid_search.best_score_\n",
        "    tuned_parameters = grid_search.best_params_\n",
        "    runtime = end_time - start_time\n",
        "\n",
        "    results.append({\n",
        "        'final_model': best_model,\n",
        "        'model': model_name,\n",
        "        'train_acc': train_acc,\n",
        "        'tuned_parameters': tuned_parameters,\n",
        "        'runtime': runtime\n",
        "    })\n",
        "\n",
        "    # Fit tuned model on full dataset\n",
        "    print(f\"Fitting final {model_name} model on full dataset...\")\n",
        "    final_model = best_model\n",
        "    final_model.fit(train_labeled.drop(columns=['class']), train_labeled['class'])\n",
        "    repeated_acc = final_model.score(train_labeled.drop(columns=['class']), train_labeled['class'])\n",
        "    results[-1]['repeated_acc'] = repeated_acc\n",
        "    # Evaluate the model on the test set\n",
        "    test_acc, f1, precision, recall, test_conf_matrix = evaluate_model_on_test(best_model, test_labeled.drop(columns=['class']), test_labeled['class'])\n",
        "    results[-1]['test_acc'] = test_acc\n",
        "    results[-1]['f1'] = f1\n",
        "    results[-1]['precision'] = precision\n",
        "    results[-1]['recall'] = recall\n",
        "    results[-1]['test_conf_matrix'] = test_conf_matrix"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JP5bcB3OMYw2",
        "outputId": "1f522273-d06a-4aa5-9111-390b351a7d07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training xgb...\n",
            "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting final xgb model on full dataset...\n",
            "Training nb...\n",
            "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
            "Fitting final nb model on full dataset...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert results to DataFrame\n",
        "df_train_results = pd.DataFrame(results, columns=[\"final_model\", \"model\", \"train_acc\", \"tuned_parameters\", \"runtime\", \"repeated_acc\", \"test_acc\", \"f1\", \"precision\", \"recall\", \"test_conf_matrix\"])\n",
        "\n",
        "# Display results\n",
        "print(df_train_results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ARoIcUp9MakL",
        "outputId": "87a3a6a7-ef71-4532-a8d7-3e28ef39946b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                         final_model model  train_acc  \\\n",
            "0  XGBClassifier(base_score=None, booster=None, c...   xgb   0.527273   \n",
            "1                           MultinomialNB(alpha=0.1)    nb   0.528375   \n",
            "\n",
            "                                    tuned_parameters      runtime  \\\n",
            "0  {'learning_rate': 0.1, 'max_depth': 15, 'n_est...  1299.602540   \n",
            "1                                     {'alpha': 0.1}     1.772867   \n",
            "\n",
            "   repeated_acc  test_acc        f1  precision    recall  \\\n",
            "0      0.975207  0.539648  0.539115   0.550847  0.539648   \n",
            "1      0.890909  0.537445  0.536650   0.540238  0.537445   \n",
            "\n",
            "                              test_conf_matrix  \n",
            "0  [[29, 25, 16], [10, 108, 73], [3, 82, 108]]  \n",
            "1   [[32, 24, 14], [14, 97, 80], [6, 72, 115]]  \n"
          ]
        }
      ]
    }
  ]
}