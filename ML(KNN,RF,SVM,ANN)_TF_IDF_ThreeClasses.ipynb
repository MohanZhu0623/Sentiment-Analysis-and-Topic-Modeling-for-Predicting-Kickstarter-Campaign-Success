{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO02SRLrAH1ZxlpSywsAncB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MohanZhu0623/Sentiment_Analysis/blob/main/ML(KNN%2CRF%2CSVM%2CANN)_TF_IDF_ThreeClasses.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dni2OIGD7hzh"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import string\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, RepeatedKFold\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim\n",
        "from gensim.models import Word2Vec"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_3Pm53oTYJAA",
        "outputId": "9c528666-0733-4694-bfbd-ca4ec993c228"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.11.4)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (7.0.4)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim) (1.14.1)\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install xgboost\n",
        "from xgboost import XGBClassifier"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4GJ1TiNBZ6hG",
        "outputId": "4b59461f-961a-437a-bfb9-4334b1980db1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from xgboost) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from xgboost) (1.11.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4kIFfUvRCt9O",
        "outputId": "e526913c-129c-4d53-e8dd-2b4f976e793f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_excel('/python_labelled_data.xlsx')"
      ],
      "metadata": {
        "id": "lYgerucdASMM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess data\n",
        "def pre_process_data(dataset):\n",
        "    # Convert to lowercase\n",
        "    dataset['text'] = dataset['text'].str.lower()\n",
        "    # Remove numbers\n",
        "    dataset['text'] = dataset['text'].str.replace(r'\\d+', '', regex=True)\n",
        "    # Remove punctuation\n",
        "    dataset['text'] = dataset['text'].str.replace(f\"[{string.punctuation}]\", \"\", regex=True)\n",
        "    # Remove whitespace\n",
        "    dataset['text'] = dataset['text'].str.strip()\n",
        "    # Remove stopwords except 'not'\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    stop_words.remove('not')\n",
        "    dataset['text'] = dataset['text'].apply(lambda x: ' '.join([word for word in word_tokenize(x) if word not in stop_words]))\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "EdYSmK3UB_Oh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply preprocessing to the data\n",
        "data = pre_process_data(data)\n",
        "print(data.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XTdOVkUeCM4c",
        "outputId": "5087f138-5ac4-4e49-e4ad-e96574cee08e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                text  class\n",
            "0  project designed help protect environment usin...      1\n",
            "1  help us built sustainable studio eliminate cla...      1\n",
            "2  paint something dont want explain isbob ross b...      0\n",
            "3  free app allow pool reservations others get gr...      1\n",
            "4  prohibition themed gastro pub dark silent head...      1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create TF-IDF vectorizer for unigrams and bigrams\n",
        "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 2), min_df=3)\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(data['text'])\n",
        "\n",
        "# Convert to DataFrame\n",
        "corpus_clean = pd.DataFrame(X_tfidf.toarray(), columns=tfidf_vectorizer.get_feature_names_out())"
      ],
      "metadata": {
        "id": "A0G6-NF7EXS3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine class label with features\n",
        "labeled_dtm = pd.concat([data[['class']], corpus_clean], axis=1)"
      ],
      "metadata": {
        "id": "C2r5PhBkYk2g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "labeled_dtm = labeled_dtm.loc[:, ~labeled_dtm.columns.duplicated()]\n",
        "\n",
        "print(labeled_dtm.columns.duplicated().sum())\n",
        "class_column = labeled_dtm['class'].squeeze()\n",
        "print(class_column.shape)\n",
        "print(class_column.value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iohIZ4CAkz4K",
        "outputId": "4565d084-7a48-4a4d-de3b-f22f699bf59f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "(2269,)\n",
            "class\n",
            " 1    962\n",
            " 0    955\n",
            "-1    352\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the labeled DTM into training set (80%) and hold-out test set (20%)\n",
        "partition = 0.8\n",
        "train_labeled, test_labeled = train_test_split(labeled_dtm, test_size=1-partition, random_state=128, stratify=labeled_dtm['class'])"
      ],
      "metadata": {
        "id": "DHm0KcptE1HD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Training set size: {train_labeled.shape}\")\n",
        "print(f\"Test set size: {test_labeled.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3-TLdGNxE3XP",
        "outputId": "de5063ba-d691-4f38-95d0-ada54671bba2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set size: (1815, 2328)\n",
            "Test set size: (454, 2328)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set cross-validation parameters\n",
        "cv_tune = 5\n",
        "rep_tune = 1\n",
        "cv_final = 10\n",
        "rep_final = 5\n",
        "\n",
        "# Define cross-validation strategies\n",
        "cv_strategy_tune = RepeatedKFold(n_splits=cv_tune, n_repeats=rep_tune, random_state=128)\n",
        "cv_strategy_final = RepeatedKFold(n_splits=cv_final, n_repeats=rep_final, random_state=128)"
      ],
      "metadata": {
        "id": "gxK0RSGVFCR7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define parameter grids\n",
        "param_grid_knn = {'n_neighbors': [1, 15, 30, 45, 65]}\n",
        "param_grid_rf = {'n_estimators': [100], 'max_features': [round(np.sqrt(train_labeled.shape[1]) / 2), round(np.sqrt(train_labeled.shape[1]))], 'min_samples_split': [2]}\n",
        "param_grid_svm = {'C': [0.01, 0.1, 1, 10, 100]}\n",
        "param_grid_ann = {'hidden_layer_sizes': [(50, 50), (100,)], 'activation': ['tanh', 'relu'], 'solver': ['sgd', 'adam']}\n",
        "\n",
        "\n",
        "# Define models\n",
        "knn = KNeighborsClassifier()\n",
        "rf = RandomForestClassifier()\n",
        "svm = SVC()\n",
        "ann = MLPClassifier(max_iter=1000)\n",
        "\n",
        "# Combine models and parameter grids\n",
        "models = {\n",
        "    'knn': (knn, param_grid_knn),\n",
        "    'rf': (rf, param_grid_rf),\n",
        "    'svm': (svm, param_grid_svm),\n",
        "    'ann': (ann, param_grid_ann),\n",
        "}\n",
        "\n",
        "# Initialize results dataframe\n",
        "results = []\n",
        "\n",
        "\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score\n",
        "# Define a function to evaluate the model on the test set\n",
        "def evaluate_model_on_test(model, X_test, y_test):\n",
        "    predictions = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, predictions)\n",
        "    f1 = f1_score(y_test, predictions, average='weighted')\n",
        "    precision = precision_score(y_test, predictions, average='weighted')\n",
        "    recall = recall_score(y_test, predictions, average='weighted')\n",
        "    conf_matrix = confusion_matrix(y_test, predictions)\n",
        "    return accuracy, f1, precision, recall, conf_matrix\n"
      ],
      "metadata": {
        "id": "bP8YFXclV4az"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train and tune models\n",
        "for model_name, (model, param_grid) in models.items():\n",
        "    print(f\"Training {model_name}...\")\n",
        "    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=cv_strategy_tune, verbose=2, n_jobs=-1)\n",
        "    start_time = time.time()\n",
        "    grid_search.fit(train_labeled.drop(columns=['class']), train_labeled['class'])\n",
        "    end_time = time.time()\n",
        "\n",
        "    # Save results\n",
        "    best_model = grid_search.best_estimator_\n",
        "    train_acc = grid_search.best_score_\n",
        "    tuned_parameters = grid_search.best_params_\n",
        "    runtime = end_time - start_time\n",
        "\n",
        "    results.append({\n",
        "        'final_model': best_model,\n",
        "        'model': model_name,\n",
        "        'train_acc': train_acc,\n",
        "        'tuned_parameters': tuned_parameters,\n",
        "        'runtime': runtime\n",
        "    })\n",
        "\n",
        "    # Fit tuned model on full dataset\n",
        "    print(f\"Fitting final {model_name} model on full dataset...\")\n",
        "    final_model = model\n",
        "    final_model.fit(labeled_dtm.drop(columns=['class']), labeled_dtm['class'])\n",
        "    repeated_acc = final_model.score(labeled_dtm.drop(columns=['class']), labeled_dtm['class'])\n",
        "    results[-1]['repeated_acc'] = repeated_acc\n",
        "\n",
        "    # Evaluate the model on the test set\n",
        "    test_acc, f1, precision, recall, test_conf_matrix = evaluate_model_on_test(best_model, test_labeled.drop(columns=['class']), test_labeled['class'])\n",
        "    results[-1]['test_acc'] = test_acc\n",
        "    results[-1]['f1'] = f1\n",
        "    results[-1]['precision'] = precision\n",
        "    results[-1]['recall'] = recall\n",
        "    results[-1]['test_conf_matrix'] = test_conf_matrix"
      ],
      "metadata": {
        "id": "Cz-hRRa0Gkvu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfe0876c-cf64-4484-d314-7243f4f409a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training knn...\n",
            "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
            "Fitting final knn model on full dataset...\n",
            "Training rf...\n",
            "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
            "Fitting final rf model on full dataset...\n",
            "Training svm...\n",
            "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
            "Fitting final svm model on full dataset...\n",
            "Training ann...\n",
            "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
            "Fitting final ann model on full dataset...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert results to DataFrame\n",
        "df_train_results = pd.DataFrame(results, columns=[\"final_model\", \"model\", \"train_acc\", \"tuned_parameters\", \"runtime\", \"repeated_acc\", \"test_acc\", \"f1\", \"precision\", \"recall\", \"test_conf_matrix\"])\n",
        "\n",
        "# Display results\n",
        "print(df_train_results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_1wi1kwDHJj2",
        "outputId": "6bc8dacd-d5d6-4aa7-8b6a-65d3edfb4823"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                         final_model model  train_acc  \\\n",
            "0               KNeighborsClassifier(n_neighbors=30)   knn   0.514050   \n",
            "1  (DecisionTreeClassifier(max_features=24, rando...    rf   0.538292   \n",
            "2                                          SVC(C=10)   svm   0.520661   \n",
            "3  MLPClassifier(hidden_layer_sizes=(50, 50), max...   ann   0.519008   \n",
            "\n",
            "                                    tuned_parameters      runtime  \\\n",
            "0                                {'n_neighbors': 30}     6.000789   \n",
            "1  {'max_features': 24, 'min_samples_split': 2, '...    24.877025   \n",
            "2                                          {'C': 10}   167.801672   \n",
            "3  {'activation': 'relu', 'hidden_layer_sizes': (...  1436.745502   \n",
            "\n",
            "   repeated_acc  test_acc        f1  precision    recall  \\\n",
            "0      0.687087  0.522026  0.515274   0.533110  0.522026   \n",
            "1      0.988100  0.530837  0.526831   0.562224  0.530837   \n",
            "2      0.972234  0.513216  0.509543   0.529292  0.513216   \n",
            "3      0.988100  0.515419  0.515151   0.517881  0.515419   \n",
            "\n",
            "                              test_conf_matrix  \n",
            "0  [[19, 35, 16], [10, 103, 78], [1, 77, 115]]  \n",
            "1   [[25, 35, 10], [6, 125, 60], [1, 101, 91]]  \n",
            "2  [[22, 31, 17], [10, 100, 81], [0, 82, 111]]  \n",
            "3   [[30, 27, 13], [20, 96, 75], [4, 81, 108]]  \n"
          ]
        }
      ]
    }
  ]
}