{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM4KOEaYOA47xavrIDZGHoI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MohanZhu0623/Sentiment_Analysis/blob/main/%E2%80%9CML(KNN%2CRF%2CSVM%2CANN%2CXGBoost%2CNB)_TF_IDF_TwoClasses.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dni2OIGD7hzh"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import string\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, RepeatedKFold\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split, RepeatedKFold\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix"
      ],
      "metadata": {
        "id": "bX_xZVweXgW3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim\n",
        "from gensim.models import Word2Vec"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_3Pm53oTYJAA",
        "outputId": "e5d7da98-d5e7-4df2-fdf2-2d9aab508b9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.11.4)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (7.0.4)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim) (1.14.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install xgboost\n",
        "from xgboost import XGBClassifier"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4GJ1TiNBZ6hG",
        "outputId": "0b60160d-e1c4-44d7-98d3-28c45c5224d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from xgboost) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from xgboost) (1.11.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4kIFfUvRCt9O",
        "outputId": "6f5b523a-e8df-49db-f7b7-aa802f07a3e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_excel('/content/binary_labelled _data.xlsx')"
      ],
      "metadata": {
        "id": "lYgerucdASMM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess data\n",
        "def pre_process_data(dataset):\n",
        "    # Convert to lowercase\n",
        "    dataset['text'] = dataset['text'].str.lower()\n",
        "    # Remove numbers\n",
        "    dataset['text'] = dataset['text'].str.replace(r'\\d+', '', regex=True)\n",
        "    # Remove punctuation\n",
        "    dataset['text'] = dataset['text'].str.replace(f\"[{string.punctuation}]\", \"\", regex=True)\n",
        "    # Remove whitespace\n",
        "    dataset['text'] = dataset['text'].str.strip()\n",
        "    # Remove stopwords except 'not'\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    stop_words.remove('not')\n",
        "    dataset['text'] = dataset['text'].apply(lambda x: ' '.join([word for word in word_tokenize(x) if word not in stop_words]))\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "EdYSmK3UB_Oh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply preprocessing to the data\n",
        "data = pre_process_data(data)\n",
        "print(data.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XTdOVkUeCM4c",
        "outputId": "6987448a-76bf-42b8-d921-8f7c959535ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                text  class\n",
            "0  project designed help protect environment usin...      1\n",
            "1  help us built sustainable studio eliminate cla...      1\n",
            "2  free app allow pool reservations others get gr...      1\n",
            "3  prohibition themed gastro pub dark silent head...      1\n",
            "4  sean naturally talented trumpet player ryan wo...      1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create TF-IDF vectorizer for unigrams and bigrams\n",
        "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 2), min_df=3)\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(data['text'])\n",
        "\n",
        "# Convert to DataFrame\n",
        "corpus_clean = pd.DataFrame(X_tfidf.toarray(), columns=tfidf_vectorizer.get_feature_names_out())"
      ],
      "metadata": {
        "id": "A0G6-NF7EXS3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine class label with features\n",
        "labeled_dtm = pd.concat([data[['class']], corpus_clean], axis=1)"
      ],
      "metadata": {
        "id": "C2r5PhBkYk2g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "labeled_dtm = labeled_dtm.loc[:, ~labeled_dtm.columns.duplicated()]\n",
        "\n",
        "print(labeled_dtm.columns.duplicated().sum())\n",
        "class_column = labeled_dtm['class'].squeeze()\n",
        "print(class_column.shape)\n",
        "print(class_column.value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iohIZ4CAkz4K",
        "outputId": "dd94ac49-d52a-4ef8-ba71-a25e6bab97b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "(1314,)\n",
            "class\n",
            "1    962\n",
            "0    352\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the labeled DTM into training set (80%) and hold-out test set (20%)\n",
        "partition = 0.8\n",
        "train_labeled, test_labeled = train_test_split(labeled_dtm, test_size=1-partition, random_state=128, stratify=labeled_dtm['class'])"
      ],
      "metadata": {
        "id": "DHm0KcptE1HD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Training set size: {train_labeled.shape}\")\n",
        "print(f\"Test set size: {test_labeled.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3-TLdGNxE3XP",
        "outputId": "0cb25f91-d67c-4493-9847-976fa716a6f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set size: (1051, 1421)\n",
            "Test set size: (263, 1421)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set cross-validation parameters\n",
        "cv_tune = 5\n",
        "rep_tune = 1\n",
        "cv_final = 10\n",
        "rep_final = 5\n",
        "\n",
        "# Define cross-validation strategies\n",
        "cv_strategy_tune = RepeatedKFold(n_splits=cv_tune, n_repeats=rep_tune, random_state=128)\n",
        "cv_strategy_final = RepeatedKFold(n_splits=cv_final, n_repeats=rep_final, random_state=128)"
      ],
      "metadata": {
        "id": "gxK0RSGVFCR7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define parameter grids for all models\n",
        "param_grid_xgb = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [5, 10, 15],\n",
        "    'learning_rate': [0.01, 0.1, 0.2]\n",
        "}\n",
        "param_grid_nb = {\n",
        "    'alpha': [0.1, 0.5, 1.0, 5.0, 10.0]\n",
        "}\n",
        "param_grid_knn = {'n_neighbors': [1, 15, 30, 45, 65]}\n",
        "param_grid_rf = {'n_estimators': [100], 'max_features': [round(np.sqrt(train_labeled.shape[1]) / 2), round(np.sqrt(train_labeled.shape[1]))], 'min_samples_split': [2]}\n",
        "param_grid_svm = {'C': [0.01, 0.1, 1, 10, 100]}\n",
        "param_grid_ann = {'hidden_layer_sizes': [(50, 50), (100,)], 'activation': ['tanh', 'relu'], 'solver': ['sgd', 'adam']}\n",
        "\n",
        "# Initialize models\n",
        "xgb = XGBClassifier()\n",
        "nb = MultinomialNB()\n",
        "knn = KNeighborsClassifier()\n",
        "rf = RandomForestClassifier()\n",
        "svm = SVC()\n",
        "ann = MLPClassifier(max_iter=1000)\n",
        "\n",
        "# Combine models and parameter grids\n",
        "models = {\n",
        "    'xgb': (xgb, param_grid_xgb),\n",
        "    'nb': (nb, param_grid_nb),\n",
        "    'knn': (knn, param_grid_knn),\n",
        "    'rf': (rf, param_grid_rf),\n",
        "    'svm': (svm, param_grid_svm),\n",
        "    'ann': (ann, param_grid_ann)\n",
        "}\n",
        "\n",
        "# Initialize results list\n",
        "results = []\n",
        "\n",
        "# Define a function to evaluate the model on the test set\n",
        "def evaluate_model_on_test(model, X_test, y_test):\n",
        "    predictions = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, predictions)\n",
        "    f1 = f1_score(y_test, predictions, average='weighted')\n",
        "    precision = precision_score(y_test, predictions, average='weighted')\n",
        "    recall = recall_score(y_test, predictions, average='weighted')\n",
        "    conf_matrix = confusion_matrix(y_test, predictions)\n",
        "    return accuracy, f1, precision, recall, conf_matrix\n"
      ],
      "metadata": {
        "id": "bP8YFXclV4az"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train and tune models\n",
        "for model_name, (model, param_grid) in models.items():\n",
        "    print(f\"Training {model_name}...\")\n",
        "    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=cv_strategy_tune, verbose=2, n_jobs=-1)\n",
        "    start_time = time.time()\n",
        "    grid_search.fit(train_labeled.drop(columns=['class']), train_labeled['class'])\n",
        "    end_time = time.time()\n",
        "\n",
        "    # Save results\n",
        "    best_model = grid_search.best_estimator_\n",
        "    train_acc = grid_search.best_score_\n",
        "    tuned_parameters = grid_search.best_params_\n",
        "    runtime = end_time - start_time\n",
        "\n",
        "    results.append({\n",
        "        'final_model': best_model,\n",
        "        'model': model_name,\n",
        "        'train_acc': train_acc,\n",
        "        'tuned_parameters': tuned_parameters,\n",
        "        'runtime': runtime\n",
        "    })\n",
        "\n",
        "    # Fit tuned model on full dataset\n",
        "    print(f\"Fitting final {model_name} model on full dataset...\")\n",
        "    final_model = best_model\n",
        "    final_model.fit(train_labeled.drop(columns=['class']), train_labeled['class'])\n",
        "    repeated_acc = final_model.score(train_labeled.drop(columns=['class']), train_labeled['class'])\n",
        "    results[-1]['repeated_acc'] = repeated_acc\n",
        "\n",
        "    # Evaluate the model on the test set\n",
        "    test_acc, f1, precision, recall, test_conf_matrix = evaluate_model_on_test(best_model, test_labeled.drop(columns=['class']), test_labeled['class'])\n",
        "    results[-1]['test_acc'] = test_acc\n",
        "    results[-1]['f1'] = f1\n",
        "    results[-1]['precision'] = precision\n",
        "    results[-1]['recall'] = recall\n",
        "    results[-1]['test_conf_matrix'] = test_conf_matrix"
      ],
      "metadata": {
        "id": "Cz-hRRa0Gkvu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7538f66b-3405-456d-d4a1-c1a38aa7370d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training xgb...\n",
            "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
            "Fitting final xgb model on full dataset...\n",
            "Training nb...\n",
            "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
            "Fitting final nb model on full dataset...\n",
            "Training knn...\n",
            "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
            "Fitting final knn model on full dataset...\n",
            "Training rf...\n",
            "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
            "Fitting final rf model on full dataset...\n",
            "Training svm...\n",
            "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
            "Fitting final svm model on full dataset...\n",
            "Training ann...\n",
            "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
            "Fitting final ann model on full dataset...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert results to DataFrame\n",
        "df_train_results = pd.DataFrame(results, columns=[\"final_model\", \"model\", \"train_acc\", \"tuned_parameters\", \"runtime\", \"repeated_acc\", \"test_acc\", \"f1\", \"precision\", \"recall\", \"test_conf_matrix\"])\n",
        "\n",
        "# Display results\n",
        "print(df_train_results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_1wi1kwDHJj2",
        "outputId": "3c30e669-5ba7-4d1c-ba6d-41550e07d63b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                         final_model model  train_acc  \\\n",
            "0  XGBClassifier(base_score=None, booster=None, c...   xgb   0.775450   \n",
            "1                           MultinomialNB(alpha=0.5)    nb   0.834457   \n",
            "2               KNeighborsClassifier(n_neighbors=30)   knn   0.798312   \n",
            "3  (DecisionTreeClassifier(max_features=19, rando...    rf   0.805908   \n",
            "4                                          SVC(C=10)   svm   0.800212   \n",
            "5  MLPClassifier(hidden_layer_sizes=(50, 50), max...   ann   0.805895   \n",
            "\n",
            "                                    tuned_parameters     runtime  \\\n",
            "0  {'learning_rate': 0.1, 'max_depth': 5, 'n_esti...  293.767819   \n",
            "1                                     {'alpha': 0.5}    0.911430   \n",
            "2                                {'n_neighbors': 30}    1.758741   \n",
            "3  {'max_features': 19, 'min_samples_split': 2, '...    9.350955   \n",
            "4                                          {'C': 10}   17.048327   \n",
            "5  {'activation': 'relu', 'hidden_layer_sizes': (...  155.364710   \n",
            "\n",
            "   repeated_acc  test_acc        f1  precision    recall  \\\n",
            "0      0.906755  0.783270  0.748797   0.772909  0.783270   \n",
            "1      0.933397  0.836502  0.822313   0.833735  0.836502   \n",
            "2      0.815414  0.809886  0.778475   0.818332  0.809886   \n",
            "3      1.000000  0.825095  0.804124   0.826329  0.825095   \n",
            "4      1.000000  0.832700  0.815918   0.831664  0.832700   \n",
            "5      1.000000  0.828897  0.821831   0.821569  0.828897   \n",
            "\n",
            "        test_conf_matrix  \n",
            "0   [[21, 49], [8, 185]]  \n",
            "1   [[35, 35], [8, 185]]  \n",
            "2   [[24, 46], [4, 189]]  \n",
            "3   [[30, 40], [6, 187]]  \n",
            "4   [[33, 37], [7, 186]]  \n",
            "5  [[40, 30], [15, 178]]  \n"
          ]
        }
      ]
    }
  ]
}